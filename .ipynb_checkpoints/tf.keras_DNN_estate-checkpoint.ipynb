{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['No', 'X1 transaction date', 'X2 house age',\n",
      "       'X3 distance to the nearest MRT station',\n",
      "       'X4 number of convenience stores', 'X5 latitude', 'X6 longitude',\n",
      "       'Y house price of unit area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import ExcelFile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers  #모듈(변수나 함수를 포함)만 불러오기\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  #표준 정규화, 최대-최소 정규화\n",
    "\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel('Real estate valuation data set.xlsx')\n",
    "#df = pd.read_excel('File.xlsx', sheetname='Sheet1') #sheet명도 지정할 수 있음\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)  #python에서 data type 확인하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[i][j] : i번째 객체의 j번째 속성 [ (i:414) , (j:0=num, 1~7=att, 8=class) ]\n",
    "\n",
    "X = df.iloc[:,1:7]  # csv에서 모든행, 1~6열만 split (pandas의 data를 number index로 slicing 하는 방법)\n",
    "y = df.iloc[:,7]    # csv에서 모든행, 7열만 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.916667</td>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.916667</td>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.583333</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.500000</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.833333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date  X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0          2012.916667          32.0                                84.87882   \n",
       "1          2012.916667          19.5                               306.59470   \n",
       "2          2013.583333          13.3                               561.98450   \n",
       "3          2013.500000          13.3                               561.98450   \n",
       "4          2012.833333           5.0                               390.56840   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "0                               10     24.98298     121.54024  \n",
       "1                                9     24.98034     121.53951  \n",
       "2                                5     24.98746     121.54391  \n",
       "3                                5     24.98746     121.54391  \n",
       "4                                5     24.97937     121.54245  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()   #pandas에서 data를 상위 5개만 확인하는 함수   cf. X.tail은 하위 5개만 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2013.148953</td>\n",
       "      <td>17.712560</td>\n",
       "      <td>1083.885689</td>\n",
       "      <td>4.094203</td>\n",
       "      <td>24.969030</td>\n",
       "      <td>121.533361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.281995</td>\n",
       "      <td>11.392485</td>\n",
       "      <td>1262.109595</td>\n",
       "      <td>2.945562</td>\n",
       "      <td>0.012410</td>\n",
       "      <td>0.015347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2012.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.382840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.932070</td>\n",
       "      <td>121.473530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2012.916667</td>\n",
       "      <td>9.025000</td>\n",
       "      <td>289.324800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.963000</td>\n",
       "      <td>121.528085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2013.166667</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>492.231300</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.971100</td>\n",
       "      <td>121.538630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2013.416667</td>\n",
       "      <td>28.150000</td>\n",
       "      <td>1454.279000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.977455</td>\n",
       "      <td>121.543305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2013.583333</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>6488.021000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>25.014590</td>\n",
       "      <td>121.566270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1 transaction date  X2 house age  \\\n",
       "count           414.000000    414.000000   \n",
       "mean           2013.148953     17.712560   \n",
       "std               0.281995     11.392485   \n",
       "min            2012.666667      0.000000   \n",
       "25%            2012.916667      9.025000   \n",
       "50%            2013.166667     16.100000   \n",
       "75%            2013.416667     28.150000   \n",
       "max            2013.583333     43.800000   \n",
       "\n",
       "       X3 distance to the nearest MRT station  \\\n",
       "count                              414.000000   \n",
       "mean                              1083.885689   \n",
       "std                               1262.109595   \n",
       "min                                 23.382840   \n",
       "25%                                289.324800   \n",
       "50%                                492.231300   \n",
       "75%                               1454.279000   \n",
       "max                               6488.021000   \n",
       "\n",
       "       X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "count                       414.000000   414.000000    414.000000  \n",
       "mean                          4.094203    24.969030    121.533361  \n",
       "std                           2.945562     0.012410      0.015347  \n",
       "min                           0.000000    24.932070    121.473530  \n",
       "25%                           1.000000    24.963000    121.528085  \n",
       "50%                           4.000000    24.971100    121.538630  \n",
       "75%                           6.000000    24.977455    121.543305  \n",
       "max                          10.000000    25.014590    121.566270  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()  #pandas에서 data의 간단한 기초통계량을 보여주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013.148953302899\n",
      "17.71256038647343\n",
      "1083.8856889130436\n",
      "4.094202898550725\n",
      "24.969030072463745\n",
      "121.53336108695667\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(X.iloc[:,i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    37.9\n",
       "1    42.2\n",
       "2    47.3\n",
       "3    54.8\n",
       "4    43.1\n",
       "Name: Y house price of unit area, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    414.000000\n",
       "mean      37.980193\n",
       "std       13.606488\n",
       "min        7.600000\n",
       "25%       27.700000\n",
       "50%       38.450000\n",
       "75%       46.600000\n",
       "max      117.500000\n",
       "Name: Y house price of unit area, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# 모든 컬럼을 유사하고 적절한 구간으로 정규화 하기\n",
    "#각 컬럼들에 min-max scaling 또는 z-score normalization 적용\n",
    "scaler = MinMaxScaler()   #StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "print(type(X_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5261308777899157\n",
      "0.40439635585555783\n",
      "0.16404674517978646\n",
      "0.40942028985507245\n",
      "0.4478922983976141\n",
      "0.6451486624597256\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_norm[0])):\n",
    "    print(X_norm[:,i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_norm))\n",
    "print(type(y))\n",
    "\n",
    "numpY = np.empty((414,1)) \n",
    "\n",
    "for i in range(414):\n",
    "    numpY[i]=y[i]\n",
    "    \n",
    "print(type(numpY))\n",
    "\n",
    "#y = np.empty((150,3)) \n",
    "# training set과 test set으로 나누기\n",
    "X_train, y_train = X_norm[0:300], numpY[0:300]\n",
    "X_test,  y_test  = X_norm[300:414], numpY[300:414]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 구조 정의하기\n",
    "model = tf.keras.Sequential()  #순차적 계층화 준비\n",
    "\n",
    "model.add(layers.Dense(12, input_shape=(6,)))  #입력 8개로부터 전달받는 12개 노드의 layer 생성\n",
    "model.add(layers.Activation('relu'))  #ReLU 활성화함수 채택\n",
    "\n",
    "model.add(layers.Dense(12))         #12개 노드의 layer 생성\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(12))         #12개 노드의 layer 생성\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "model.add(layers.Activation('linear')) #회귀모형(regression) 구축을 위해서 linear 활성함수 사용\n",
    "\n",
    "# 모델 구축하기\n",
    "model.compile(\n",
    "        loss='mse',         # mean_squared_error(평균제곱오차)의 alias\n",
    "        optimizer='adam',   # 최적화 기법 중 하나\n",
    "        metrics=['mae']) # mean_absolute_error(평균절대오차)의 alias\n",
    "            # 실험 후 관찰하고 싶은 metric 들을 나열함. 단. loss에 사용된 metric은 기본적으로 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 1s 4ms/sample - loss: 1637.4262 - mae: 38.1757 - val_loss: 1581.6728 - val_mae: 36.6123\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 1611.3094 - mae: 37.8424 - val_loss: 1554.1924 - val_mae: 36.2409\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 1575.2302 - mae: 37.3781 - val_loss: 1512.5378 - val_mae: 35.6693\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 0s 208us/sample - loss: 1519.0406 - mae: 36.6279 - val_loss: 1445.7320 - val_mae: 34.7346\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 1429.9660 - mae: 35.4268 - val_loss: 1344.6489 - val_mae: 33.2644\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 0s 200us/sample - loss: 1295.8139 - mae: 33.5087 - val_loss: 1193.0740 - val_mae: 30.9182\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 1101.1340 - mae: 30.4892 - val_loss: 982.2355 - val_mae: 27.3009\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 840.2850 - mae: 25.9575 - val_loss: 715.6775 - val_mae: 21.8501\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 538.6273 - mae: 19.4877 - val_loss: 434.7096 - val_mae: 14.9858\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 282.1176 - mae: 13.4526 - val_loss: 277.3431 - val_mae: 10.9345\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 181.6364 - mae: 10.9498 - val_loss: 246.3767 - val_mae: 10.3532\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 0s 196us/sample - loss: 163.1638 - mae: 10.4989 - val_loss: 239.7776 - val_mae: 10.3941\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 0s 200us/sample - loss: 155.9030 - mae: 10.2759 - val_loss: 233.8647 - val_mae: 10.2161\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 0s 196us/sample - loss: 149.6557 - mae: 10.0302 - val_loss: 228.3746 - val_mae: 9.9690\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 143.9284 - mae: 9.8157 - val_loss: 223.8485 - val_mae: 9.8395\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 138.4411 - mae: 9.6725 - val_loss: 219.3306 - val_mae: 9.8637\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 132.5556 - mae: 9.4766 - val_loss: 214.4324 - val_mae: 9.6497\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 127.5454 - mae: 9.2211 - val_loss: 209.2058 - val_mae: 9.4319\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 121.6266 - mae: 9.0160 - val_loss: 204.3417 - val_mae: 9.2712\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 0s 200us/sample - loss: 116.2551 - mae: 8.7580 - val_loss: 200.2234 - val_mae: 9.1290\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 111.5022 - mae: 8.5982 - val_loss: 195.9215 - val_mae: 9.0102\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 0s 208us/sample - loss: 107.2782 - mae: 8.3789 - val_loss: 191.4035 - val_mae: 8.6404\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 102.2343 - mae: 8.1378 - val_loss: 188.6870 - val_mae: 8.8117\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 98.5865 - mae: 8.0592 - val_loss: 185.5258 - val_mae: 8.7159\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 95.2348 - mae: 7.7131 - val_loss: 181.1021 - val_mae: 8.2169\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 90.9720 - mae: 7.5648 - val_loss: 178.9361 - val_mae: 8.3254\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 88.4547 - mae: 7.4840 - val_loss: 176.0527 - val_mae: 8.1567\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 85.0134 - mae: 7.2617 - val_loss: 174.0787 - val_mae: 8.0874\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 82.7203 - mae: 7.1224 - val_loss: 172.1856 - val_mae: 8.0142\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 80.6209 - mae: 7.0254 - val_loss: 169.8833 - val_mae: 7.8506\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 78.7044 - mae: 6.8536 - val_loss: 167.6013 - val_mae: 7.6087\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 77.1845 - mae: 6.7961 - val_loss: 167.2572 - val_mae: 7.7668\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 76.3094 - mae: 6.7200 - val_loss: 166.7773 - val_mae: 7.8206\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 74.4051 - mae: 6.6719 - val_loss: 165.4149 - val_mae: 7.7353\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 73.3143 - mae: 6.6047 - val_loss: 164.0136 - val_mae: 7.6295\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 72.6274 - mae: 6.5834 - val_loss: 162.7207 - val_mae: 7.5388\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 71.6516 - mae: 6.4965 - val_loss: 161.6854 - val_mae: 7.4495\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 72.0032 - mae: 6.5079 - val_loss: 163.9474 - val_mae: 7.7789\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 70.6097 - mae: 6.4557 - val_loss: 160.3566 - val_mae: 7.3782\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 70.5344 - mae: 6.4444 - val_loss: 160.9002 - val_mae: 7.5089\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 69.2960 - mae: 6.3606 - val_loss: 160.0764 - val_mae: 7.4434\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 0s 221us/sample - loss: 69.2546 - mae: 6.3561 - val_loss: 161.3930 - val_mae: 7.6348\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 0s 213us/sample - loss: 69.6301 - mae: 6.3285 - val_loss: 159.3324 - val_mae: 7.4198\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 69.0855 - mae: 6.4373 - val_loss: 160.2821 - val_mae: 7.5667\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 68.8480 - mae: 6.2560 - val_loss: 158.9812 - val_mae: 7.4259\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 68.2879 - mae: 6.3194 - val_loss: 159.0040 - val_mae: 7.4506\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 67.7177 - mae: 6.2305 - val_loss: 158.7175 - val_mae: 7.4377\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 68.0099 - mae: 6.2527 - val_loss: 159.0239 - val_mae: 7.4855\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 67.3584 - mae: 6.2466 - val_loss: 159.3480 - val_mae: 7.5367\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 67.7673 - mae: 6.2334 - val_loss: 158.9894 - val_mae: 7.5038\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 68.0334 - mae: 6.2029 - val_loss: 157.8761 - val_mae: 7.3910\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 67.8402 - mae: 6.3088 - val_loss: 159.0727 - val_mae: 7.5242\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 67.6870 - mae: 6.1112 - val_loss: 157.3345 - val_mae: 7.3402\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 0s 208us/sample - loss: 67.1671 - mae: 6.1930 - val_loss: 159.8819 - val_mae: 7.6103\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 0s 217us/sample - loss: 66.7081 - mae: 6.1493 - val_loss: 157.2423 - val_mae: 7.3484\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 0s 196us/sample - loss: 67.0275 - mae: 6.1374 - val_loss: 156.9543 - val_mae: 7.3203\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 66.4376 - mae: 6.1122 - val_loss: 158.4406 - val_mae: 7.4872\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 0s 204us/sample - loss: 66.7688 - mae: 6.1748 - val_loss: 157.1088 - val_mae: 7.3504\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 0s 213us/sample - loss: 66.4191 - mae: 6.0652 - val_loss: 157.0248 - val_mae: 7.3423\n",
      "Epoch 60/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 66.5108 - mae: 6.1547 - val_loss: 158.5572 - val_mae: 7.5032\n",
      "Epoch 61/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 66.3163 - mae: 6.0834 - val_loss: 157.3986 - val_mae: 7.4017\n",
      "Epoch 62/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 66.2680 - mae: 6.0733 - val_loss: 156.7629 - val_mae: 7.3361\n",
      "Epoch 63/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 66.4094 - mae: 6.1238 - val_loss: 157.6654 - val_mae: 7.4234\n",
      "Epoch 64/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 66.2963 - mae: 6.0774 - val_loss: 157.8872 - val_mae: 7.4523\n",
      "Epoch 65/100\n",
      "240/240 [==============================] - 0s 200us/sample - loss: 66.0776 - mae: 6.0593 - val_loss: 157.2701 - val_mae: 7.3951\n",
      "Epoch 66/100\n",
      "240/240 [==============================] - 0s 375us/sample - loss: 65.8969 - mae: 6.0662 - val_loss: 157.0543 - val_mae: 7.3777\n",
      "Epoch 67/100\n",
      "240/240 [==============================] - 0s 208us/sample - loss: 66.4550 - mae: 6.1025 - val_loss: 156.6678 - val_mae: 7.3327\n",
      "Epoch 68/100\n",
      "240/240 [==============================] - 0s 154us/sample - loss: 67.0701 - mae: 6.0906 - val_loss: 157.5148 - val_mae: 7.4195\n",
      "Epoch 69/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 65.9922 - mae: 6.0400 - val_loss: 156.3763 - val_mae: 7.3020\n",
      "Epoch 70/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 65.9274 - mae: 6.0681 - val_loss: 156.2187 - val_mae: 7.2935\n",
      "Epoch 71/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 65.8365 - mae: 5.9987 - val_loss: 156.8129 - val_mae: 7.3512\n",
      "Epoch 72/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 65.6833 - mae: 6.0150 - val_loss: 157.3590 - val_mae: 7.4083\n",
      "Epoch 73/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 66.0366 - mae: 6.0766 - val_loss: 157.4041 - val_mae: 7.4167\n",
      "Epoch 74/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 66.8663 - mae: 6.0521 - val_loss: 157.5910 - val_mae: 7.4346\n",
      "Epoch 75/100\n",
      "240/240 [==============================] - 0s 213us/sample - loss: 65.5811 - mae: 6.0546 - val_loss: 156.4762 - val_mae: 7.3290\n",
      "Epoch 76/100\n",
      "240/240 [==============================] - 0s 183us/sample - loss: 65.6402 - mae: 5.9864 - val_loss: 155.6195 - val_mae: 7.2291\n",
      "Epoch 77/100\n",
      "240/240 [==============================] - 0s 171us/sample - loss: 65.6480 - mae: 6.0088 - val_loss: 157.2320 - val_mae: 7.4042\n",
      "Epoch 78/100\n",
      "240/240 [==============================] - 0s 188us/sample - loss: 65.6696 - mae: 5.9888 - val_loss: 155.4982 - val_mae: 7.2240\n",
      "Epoch 79/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 66.2909 - mae: 6.0636 - val_loss: 156.0083 - val_mae: 7.2801\n",
      "Epoch 80/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 65.6314 - mae: 6.0260 - val_loss: 156.8831 - val_mae: 7.3759\n",
      "Epoch 81/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 65.4818 - mae: 6.0016 - val_loss: 155.2257 - val_mae: 7.1895\n",
      "Epoch 82/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 65.6396 - mae: 5.9720 - val_loss: 157.2803 - val_mae: 7.4127\n",
      "Epoch 83/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 65.1425 - mae: 5.9664 - val_loss: 155.8990 - val_mae: 7.2747\n",
      "Epoch 84/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 65.2228 - mae: 5.9761 - val_loss: 156.7284 - val_mae: 7.3599\n",
      "Epoch 85/100\n",
      "240/240 [==============================] - 0s 154us/sample - loss: 65.3916 - mae: 5.9527 - val_loss: 155.8432 - val_mae: 7.2707\n",
      "Epoch 86/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 65.2028 - mae: 5.9547 - val_loss: 156.7839 - val_mae: 7.3573\n",
      "Epoch 87/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 65.0226 - mae: 5.9714 - val_loss: 157.4117 - val_mae: 7.4202\n",
      "Epoch 88/100\n",
      "240/240 [==============================] - 0s 158us/sample - loss: 64.8431 - mae: 5.9444 - val_loss: 156.0984 - val_mae: 7.2961\n",
      "Epoch 89/100\n",
      "240/240 [==============================] - 0s 154us/sample - loss: 65.0004 - mae: 5.9225 - val_loss: 155.0384 - val_mae: 7.1836\n",
      "Epoch 90/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 64.8237 - mae: 5.9241 - val_loss: 155.0964 - val_mae: 7.1896\n",
      "Epoch 91/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 64.3664 - mae: 5.9130 - val_loss: 155.8729 - val_mae: 7.2737\n",
      "Epoch 92/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 64.4155 - mae: 5.8893 - val_loss: 155.7990 - val_mae: 7.2654\n",
      "Epoch 93/100\n",
      "240/240 [==============================] - 0s 200us/sample - loss: 64.4749 - mae: 5.9446 - val_loss: 155.9811 - val_mae: 7.2889\n",
      "Epoch 94/100\n",
      "240/240 [==============================] - 0s 167us/sample - loss: 64.8954 - mae: 5.8894 - val_loss: 155.2748 - val_mae: 7.2138\n",
      "Epoch 95/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 64.5840 - mae: 5.9334 - val_loss: 155.8824 - val_mae: 7.2819\n",
      "Epoch 96/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 64.3138 - mae: 5.9026 - val_loss: 156.1180 - val_mae: 7.3015\n",
      "Epoch 97/100\n",
      "240/240 [==============================] - 0s 179us/sample - loss: 64.8961 - mae: 5.8740 - val_loss: 156.3268 - val_mae: 7.3165\n",
      "Epoch 98/100\n",
      "240/240 [==============================] - 0s 175us/sample - loss: 64.4673 - mae: 5.8909 - val_loss: 154.6447 - val_mae: 7.1579\n",
      "Epoch 99/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 63.8904 - mae: 5.8597 - val_loss: 154.8607 - val_mae: 7.1746\n",
      "Epoch 100/100\n",
      "240/240 [==============================] - 0s 163us/sample - loss: 63.9306 - mae: 5.8501 - val_loss: 154.4609 - val_mae: 7.1331\n",
      "114/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 684us/sample - loss: 58.2981 - mae: 6.2403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:  67.86319351196289\n",
      "test_mae:  6.240308\n",
      "Saved model to disk.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 훈련하기\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=10,    #10개에 한 번씩 업데이터 실행\n",
    "    epochs=100,       #훈련 데이터셋을 최대 500회 반복 실험. 조기중지될 수 있음.\n",
    "    validation_split=0.2,  \n",
    "      #validation data 분할 비율. 즉, 300개 중에서 20%인 60개는 validation용으로 사용\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)],  \n",
    "      #'val_loss'를 monitor하여 20회 동안 개선이 되지 않으면 조기중지\n",
    "    verbose=1)  #전 과정을 화면에 출력(1) 또는 미출력(0) 모드\n",
    "\n",
    "# 테스트 데이터로 평가하기\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print('test_loss: ', scores[0])\n",
    "print('test_mae: ', scores[1])\n",
    "\n",
    "# 나중을 위해서 모형 저장해두기\n",
    "model.save(\"dnn_estate.h5\")\n",
    "print(\"Saved model to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "mae\n",
      "val_loss\n",
      "val_mae\n"
     ]
    }
   ],
   "source": [
    "for i in hist.history:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 409\n",
      "Trainable params: 409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 67.86\n",
      "mae: 6.24\n"
     ]
    }
   ],
   "source": [
    "# 관찰된 metric 값들을 확인함\n",
    "for i in range(len(scores)):\n",
    "    print(\"%s: %.2f\" % (model.metrics_names[i], scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')   # 훈련데이터의 loss (즉, mse)\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss') # 검증데이터의 loss (즉, mse)\n",
    "\n",
    "acc_ax.plot(hist.history['mae'], 'b', label='train mae')   # 훈련데이터의 mae\n",
    "acc_ax.plot(hist.history['val_mae'], 'g', label='val mae') # 검증데이터의 mae\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('mean_absolute_error')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **예측결과 확인해보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# test data에 대한 예측값\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(5,5))  # 그림 가로, 세로 크기\n",
    "plt.scatter(y_test, y_pred, label=\"house price per unit\", s=3)  #test data의 실제 y값과 예측 y값을 입력\n",
    "plt.title('Comparison b/w Actual and Predicted Values')\n",
    "plt.xlabel('y_actual')   # X축 이름\n",
    "plt.ylabel('y_predicted')# Y축 이름\n",
    "plt.legend()\n",
    "plt.xlim((0,100))  # X축 표시 범위\n",
    "plt.ylim((0,100))  # Y축 표시 범위\n",
    "plt.show()\n",
    "# save_fig(\"prediction result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **저장된 모형 불러와서 사용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 409\n",
      "Trainable params: 409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "114/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 44us/sample - loss: 58.2981 - mae: 6.2403\n",
      "test_loss:  67.86319351196289\n",
      "test_mse:  6.240308\n"
     ]
    }
   ],
   "source": [
    "#load and evaluate the saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "#load model\n",
    "loaded_model = load_model(\"dnn_estate.h5\")\n",
    "model.summary()\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('test_loss: ', score[0])\n",
    "print('test_mse: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **한 값만 예측해보기**\n",
    "regression은 predict() 함수를 사용해야 함.\n",
    "cf. classification은 predict_classes() 함수를 사용함. (각 class 소속 확률은 predict_proba() 함수로 확인 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45454541, 0.05707763, 0.02055202, 0.4       , 0.42280659,\n",
       "       0.71587233])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm[300]  #이미 normalization 되어있는 X값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred =  [[44.58047]]\n",
      "y_test =  36.9\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_norm[[300]])\n",
    "print('y_pred = ', y_pred)\n",
    "print('y_test = ', y[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1 transaction date                       2013.083333\n",
       "X2 house age                                 2.500000\n",
       "X3 distance to the nearest MRT station     156.244200\n",
       "X4 number of convenience stores              4.000000\n",
       "X5 latitude                                 24.966960\n",
       "X6 longitude                               121.539920\n",
       "Name: 300, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45418181 0.05707763 0.02055202 0.4        0.42280659 0.71587233]]\n"
     ]
    }
   ],
   "source": [
    "X_new = [2013.08300, 2.50000, 156.24420, 4.00000, 24.96696, 121.53992]  # normalization 되지 않은 X값\n",
    "X_new_norm = scaler.transform([X_new])   # X를 정규화할 때 사용한 scaler를 이용하여 동일하게 정규화 적용시킴\n",
    "print(X_new_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred =  [[44.579163]]\n",
      "y_actual =  [36.9]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(np.array(X_new_norm,))\n",
    "print(\"y_pred = \", y_pred)\n",
    "print(\"y_actual = \", y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
